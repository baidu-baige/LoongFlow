# LoongFlow Examples

This directory contains examples of using the LoongFlow framework for code evolution and algorithm optimization. Each subdirectory represents an independent task, demonstrating how to configure the initial code, evaluator, and task description to allow the Agent to automatically optimize the code to reach a target score.

## üöÄ Quick Start

To run an example (taking `max_to_min_ratios` as an example), please use `general_evolve_agent.py` as the entry point from the project root directory:

```bash
# Assuming you are in the evolux project root directory
export PYTHONPATH=$PYTHONPATH:.

python agents/general_evolve/general_evolve_agent.py \
    --config agents/general_evolve/examples/max_to_min_ratios/task_config.yaml \
    --initial-file agents/general_evolve/examples/max_to_min_ratios/initial_program.py \
    --eval-file agents/general_evolve/examples/max_to_min_ratios/eval_program.py
```

Alternatively, if your `task_config.yaml` already has code embedded (or if you wish to override file paths via the command line), you can use the command above. It is generally recommended to explicitly specify `--initial-file` and `--eval-file` to override default values in the configuration file.

---

## üõ†Ô∏è How to Create Custom Tasks

To create a new evolution task, you need to prepare three core files. We will use `max_to_min_ratios` as a template for explanation.

### 1. Initial Code (`initial_program.py`)

This is the code file you want the Agent to optimize.

**Specifications:**

- Import statements and helper functions are usually placed outside the core function.
- The core function should be a function that can run independently or be called.

**Example Template:**

```python
"""Task Name: e.g., Max to Min Ratios"""

import numpy as np
import scipy as sp

def optimize_construct(n=16, d=2):
    """
    Write the function's docstring here, describing input and output.
    The Agent will read this code and attempt to improve it.
    """
    # Initial implementation to be optimized (even if it's empty or performs poorly)
    points = np.zeros((n, d))
    ratio_squared = 0.0
    return points, ratio_squared


# This is an entry point for local testing; the Agent will not modify code outside the Block during evolution
if __name__ == "__main__":
    optimize_construct(16, 2)
```

### 2. Evaluator Code (`eval_program.py`)

The evaluator is responsible for running the code generated by the Agent, verifying its correctness, and calculating the score.

**Specifications:**

- Must define a function named `evaluate(program_path)`.
- This function receives the path of the generated code file as an argument.
- The **return value** must be a dictionary (Dict) containing specific fields.

**Return Value Format:**

```python
{
    "status": "success",      # or "execution_failed", "validation_failed"
    "score": float,           # Core metric: LoongFlow will try to maximize this score (usually compared with target_score)
    "summary": str,           # Short text summary for log output
    "metrics": {              # Detailed metrics for analysis
        "validity": 1.0,
        "eval_time": 0.5,
        "your_custom_metric": 123.45
    },
    "artifacts": {            # Any extra information you want to save (debug info, error stacks, result data)
        "execution_time": "0.5s",
        "points": "[...]"
    }
}
```

**Writing Suggestions:**

1.  **Dynamic Loading:** Use something like `importlib` or a custom `run_external_function` (as shown in the example) to dynamically load and run the file specified by `program_path`.
2.  **Sandbox/Timeout Control:** Ensure a timeout mechanism (like `signal.alarm` or subprocesses) is set to prevent generated infinite loops from freezing the evaluator.
3.  **Error Handling:** Catch all exceptions. If code execution fails, return `status: "execution_failed"` and `score: 0.0`.
4.  **Validation Logic:** Even if the code runs, verify if the result format is correct (e.g., is the Shape correct, are the values legal). If illegal, return `status: "validation_failed"`.

**Example Snippet:**

```python
def evaluate(program_path):
    try:
        # 1. Run the generated code
        result = run_external_function(program_path, "optimize_construct", timeout_seconds=20)

        # 2. Verify results
        if not is_valid(result):
            return {
                "status": "validation_failed",
                "score": 0.0,
                "summary": "Output shape mismatch."
            }

        # 3. Calculate score (assuming the goal is to maximize some ratio)
        score = calculate_score(result)

        return {
            "status": "success",
            "score": score,
            "summary": f"Success! Score: {score}",
            "metrics": {"raw_value": result}
        }

    except Exception as e:
        return {
            "status": "execution_failed",
            "score": 0.0,
            "summary": str(e)
        }
```

### 3. Task Configuration (`task_config.yaml`)

The configuration file defines the evolution goals, the LLM used, the Agent components used, and runtime parameters.

**Key Field Explanations:**

```yaml
# Global Configuration
workspace_path: "./output" # Output directory

# LLM Configuration (if not specified, defaults from environment variables or code are used)
llm_config:
  model: "deepseek-r1-250528"
  # ... other configs like api_key, url

# Evolution Process Configuration
evolve:
  # [Important] Task Description (Prompt)
  # This description directly tells the LLM what its goal is and what kind of code to write.
  # Must clearly define input/output formats and optimization goals.
  task: |
    You are an expert mathematician...
    Objective: Find n points...
    Input: n=16, d=2
    Output: points (np.array), ratio_squared (float)
    Target: Maximize the score...

  # Select components to use (corresponding to registered worker names in code)
  planner_name: "evolve_planner"
  executor_name: "evolve_executor_fuse" # or evolve_executor_chat, evolve_executor_react
  summary_name: "evolve_summary"

  # Evolution Parameters
  max_iterations: 1000 # Maximum iterations
  target_score: 1.0 # Target score, stop upon reaching
  concurrency: 2 # Concurrency for evaluation

  # Evaluator Configuration
  evaluator:
    timeout: 1200

  # Population/Database Configuration
  database:
    storage_type: "in_memory"
    population_size: 100
```

---

## Directory Structure Example

```text
examples/
‚îú‚îÄ‚îÄ max_to_min_ratios/              # Task directory
‚îÇ   ‚îú‚îÄ‚îÄ initial_program.py          # Contains # EVOLVE-BLOCK
‚îÇ   ‚îú‚îÄ‚îÄ eval_program.py             # Contains evaluate() function
‚îÇ   ‚îú‚îÄ‚îÄ task_config.yaml            # Contains task prompt and parameters
‚îÇ   ‚îî‚îÄ‚îÄ README.md (Optional)        # Specific instructions for this task
‚îî‚îÄ‚îÄ ... other tasks
```

## Common Troubleshooting

1.  **`ImportError` or `ModuleNotFound`**:

    - Ensure `PYTHONPATH` includes the project root directory when running `general_evolve_agent.py`.
    - When dynamically loading modules in the evaluation code, pay attention to path handling (refer to the `run_external_function` implementation in `eval_program.py`).

2.  **Score remains 0**:

    - Check if `eval_program.py` correctly captures exceptions.
    - Check the logs to confirm if generated code execution failed due to syntax errors or timeouts.
    - Confirm if the `task` description in `task_config.yaml` is clear and if the LLM understands the output format requirements.

3.  **No progress in evolution**:
    - Try changing `executor_name` (e.g., from `evolve_executor_chat` to `evolve_executor_fuse`).
    - Adjust the `task` prompt to give more algorithmic hints or specific Search Methods suggestions.
    - Check various indicators in `metrics` to see if it's stuck in a local optimum.
