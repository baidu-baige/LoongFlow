# -*- coding: utf-8 -*-
"""
LLM Generated Code
"""

import gc
import os

import numpy as np
import pandas as pd
import tensorflow as tf

from create_features import create_features
from cross_validation import cross_validation
from ensemble import ensemble
# Component imports
from load_data import load_data
from train_and_predict import PREDICTION_ENGINES

BASE_DATA_PATH = "/root/workspace/evolux/output/mlebench/denoising-dirty-documents/prepared/public"
OUTPUT_DATA_PATH = "output/f88466a1-e032-494a-acbe-a8ee4e4d23cf/9/executor/output"


def workflow() -> dict:
    """
    Orchestrates the complete end-to-end machine learning pipeline in production mode.
    
    Integrates:
    - Data Loading (Full Dataset with Stride 40)
    - Custom GroupKFold Cross-Validation (by Image ID)
    - Feature Engineering (Normalization, Reshaping to HxWx1)
    - Training: Robust U-Net with Gaussian Noise & TTA
    - Ensembling: Pixel-wise averaging across folds
    - Submission: 'Melted' pixel-format CSV generation (Incremental Write)
    
    Returns:
        dict: Contains path to the final submission file.
    """
    # -------------------------------------------------------------------------
    # 0. Setup & Configuration
    # -------------------------------------------------------------------------
    # Ensure output directory exists
    os.makedirs(OUTPUT_DATA_PATH, exist_ok=True)

    # Configure GPU Memory Growth to prevent allocation errors
    try:
        gpus = tf.config.list_physical_devices('GPU')
        if gpus:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print(f"GPU Acceleration Enabled: Found {len(gpus)} GPU(s).")
        else:
            print("WARNING: No GPU found. Pipeline will run on CPU (Significant slowdown expected).")
    except Exception as e:
        print(f"GPU Config Warning: {e}")

    # -------------------------------------------------------------------------
    # 1. Load Data
    # -------------------------------------------------------------------------
    print("Step 1: Loading full dataset...")
    # validation_mode=False ensures we load the complete dataset for production
    X, y, X_test, test_ids = load_data(validation_mode=False)
    print(f"Data Loaded: Train Patches={len(X)}, Test Images={len(X_test)}")

    # -------------------------------------------------------------------------
    # 2. Setup Cross-Validation
    # -------------------------------------------------------------------------
    print("Step 2: Initializing Cross-Validation...")
    # Returns an ImageIdGroupKFold instance to prevent data leakage between patches of same image
    cv = cross_validation(X, y)

    # Prepare storage for predictions
    # Structure: {model_name: [pred_fold1, pred_fold2, ...]}
    all_test_preds = {name: [] for name in PREDICTION_ENGINES.keys()}
    # OOF predictions dict (placeholder for compatibility/metrics)
    all_oof_preds = {name: [] for name in PREDICTION_ENGINES.keys()}

    # -------------------------------------------------------------------------
    # 3. Cross-Validation Loop
    # -------------------------------------------------------------------------
    print("Step 3: Starting Training Loop...")
    fold_num = 1

    # Iterate through folds generated by the custom splitter
    for train_idx, val_idx in cv.split(X, y):
        print(f"\n--- Processing Fold {fold_num} ---")

        # a. Split Data
        # Use .iloc because indices generated by split() are positional relative to X
        X_train_fold = X.iloc[train_idx].copy()
        y_train_fold = y.iloc[train_idx].copy()
        X_val_fold = X.iloc[val_idx].copy()
        y_val_fold = y.iloc[val_idx].copy()

        # b. Feature Engineering
        # Transforms raw images/patches into float32 (H,W,1) tensors normalized to [0,1]
        print(f"Fold {fold_num}: Applying Feature Engineering...")
        X_train_trans, y_train_trans, X_val_trans, y_val_trans, X_test_trans = create_features(
            X_train_fold, y_train_fold,
            X_val_fold, y_val_fold,
            X_test  # X_test is transformed specifically for this fold's inference
        )

        # c. Model Training & Inference
        for model_name, train_func in PREDICTION_ENGINES.items():
            print(f"Fold {fold_num}: Training model '{model_name}'...")

            # Clear Keras session to free up GPU memory from previous fold
            tf.keras.backend.clear_session()
            gc.collect()

            # Execute training (e.g., U-Net with TTA)
            # Returns predictions as pd.Series of numpy arrays
            val_preds, test_preds = train_func(
                X_train_trans, y_train_trans,
                X_val_trans, y_val_trans,
                X_test_trans
            )

            # Store predictions
            all_test_preds[model_name].append(test_preds)
            all_oof_preds[model_name].append(val_preds)

            print(f"Fold {fold_num}: {model_name} finished.")

        fold_num += 1

    # -------------------------------------------------------------------------
    # 4. Ensemble
    # -------------------------------------------------------------------------
    print("\nStep 4: Ensembling Predictions...")
    # Aggregates predictions across folds and models using pixel-wise averaging on GPU
    # y_true_full is passed as None since we are doing inference on the Test set
    final_preds_series = ensemble(all_oof_preds, all_test_preds, y_true_full=None)

    # -------------------------------------------------------------------------
    # 5. Generate Submission File
    # -------------------------------------------------------------------------
    print("Step 5: Generating Submission CSV...")
    submission_path = os.path.join(OUTPUT_DATA_PATH, "submission.csv")

    # We must format the output as "id,value" where id is "{image_id}_{row}_{col}"
    # This process creates a massive file, so we write incrementally.

    try:
        with open(submission_path, 'w') as f:
            f.write("id,value\n")

            total_imgs = len(final_preds_series)
            processed_count = 0

            # Iterate through the ensembled predictions
            # Note: final_preds_series index corresponds to the index of X_test/test_ids
            for idx, img_arr in final_preds_series.items():

                # Retrieve the actual string Image ID from the test_ids DataFrame
                # test_ids is guaranteed to align with X_test indices
                img_id_str = test_ids.loc[idx, 'image_id']

                # Handle Channel Dimension: (H, W, 1) -> (H, W)
                if img_arr.ndim == 3 and img_arr.shape[-1] == 1:
                    img_arr = img_arr[:, :, 0]
                elif img_arr.ndim == 3:
                    img_arr = np.squeeze(img_arr)

                h, w = img_arr.shape

                # Create 1-based coordinate grids for rows and columns as per task spec
                # mgrid returns (2, H, W)
                r_grid, c_grid = np.mgrid[1:h + 1, 1:w + 1]

                # Flatten arrays for CSV writing
                r_flat = r_grid.ravel()
                c_flat = c_grid.ravel()
                v_flat = img_arr.ravel()

                # Construct IDs: "{img_id}_{row}_{col}"
                ids = [f"{img_id_str}_{r}_{c}" for r, c in zip(r_flat, c_flat)]

                # Create a temporary DataFrame for this image to leverage efficient CSV export
                df_chunk = pd.DataFrame({
                    'id': ids,
                    'value': v_flat
                })

                # Append to file without header
                # float_format='%.4f' ensures 4 decimal places, sufficient for RMSE
                df_chunk.to_csv(f, header=False, index=False, float_format='%.4f')

                processed_count += 1
                if processed_count % 10 == 0:
                    print(f"Processed {processed_count}/{total_imgs} images...")
                    # Periodic garbage collection during intensive file I/O loops
                    gc.collect()

        print(f"Submission successfully saved to: {submission_path}")

    except Exception as e:
        print(f"Error generating submission file: {e}")
        raise e

    # -------------------------------------------------------------------------
    # 6. Final Return
    # -------------------------------------------------------------------------
    return {
        "submission_file_path": submission_path
    }
